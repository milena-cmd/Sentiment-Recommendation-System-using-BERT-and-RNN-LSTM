{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook\n",
    "\n",
    "This notebook loads the raw datasets, cleans the textual reviews, and stores the processed data for further analysis.\n",
    "\n",
    "Preprocessing includes removing punctuation, stopwords, correcting spelling errors, and normalizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pandas nltk textblob contractions unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DATA_PATH = \"../raw/\"\n",
    "PROCESSED_DATA_PATH = \"../processed/\"\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "datasets = {}\n",
    "for file in os.listdir(RAW_DATA_PATH):\n",
    "    if file.endswith('.csv'):\n",
    "        datasets[file.split('.')[0]] = pd.read_csv(os.path.join(RAW_DATA_PATH, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\S+|www\.\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function to all datasets\n",
    "for key in datasets.keys():\n",
    "    datasets[key]['cleaned_review'] = datasets[key]['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "for key in datasets.keys():\n",
    "    datasets[key].to_csv(os.path.join(PROCESSED_DATA_PATH, f'{key}_processed.csv'), index=False)\n",
    "\n",
    "print('âœ… Data preprocessing completed and saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


